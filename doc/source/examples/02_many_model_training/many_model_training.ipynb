{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce29a8bb",
   "metadata": {},
   "source": [
    "# Many Model Training with Ray Tune\n",
    "\n",
    "This template is a quickstart to using [Ray Tune](https://docs.ray.io/en/latest/tune/index.html) for batch inference. Ray Tune is one of many libraries under the [Ray AI Runtime](https://docs.ray.io/en/latest/ray-air/getting-started.html). See [this blog post](https://www.anyscale.com/blog/training-one-million-machine-learning-models-in-record-time-with-ray) for more information on the benefits of performing many model training with Ray!\n",
    "\n",
    "This template walks through time-series forecasting using `statsforecast`, but the framework and data format can be swapped out easily -- they are there just to help you build your own application!\n",
    "\n",
    "At a high level, this template will:\n",
    "\n",
    "1. [Define the training function for a single partition of data.](https://docs.ray.io/en/latest/tune/tutorials/tune-run.html)\n",
    "2. [Define a Tune search space to run training over many partitions of data.](https://docs.ray.io/en/latest/tune/tutorials/tune-search-spaces.html)\n",
    "3. [Extract the best model per dataset partition from the Tune experiment output.](https://docs.ray.io/en/latest/tune/examples/tune_analyze_results.html)\n",
    "\n",
    "> Slot in your code below wherever you see the ✂️ icon to build a many model training Ray application off of this template!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5417b693-09de-4dc2-9497-8194f64ebfe8",
   "metadata": {},
   "source": [
    "## Installing Dependencies\n",
    "\n",
    "First, we'll need to install necessary dependencies in the Anyscale Workspace. To do so, first open up a terminal, and follow one of the following install steps, depending on which size template you picked:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e7bd87-7696-4d72-8b37-cd0310fc8d75",
   "metadata": {
    "tags": [
     "small-template"
    ]
   },
   "source": [
    "### Install Dependencies (Small-scale Template)\n",
    "\n",
    "The small-scale template only runs on a single node (the head node), so we just need to install the requirements *locally*.\n",
    "\n",
    "```\n",
    "pip install -r requirements.txt --upgrade\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca1ca7c-5fe8-4b3b-b1a8-f4092bdfbbd6",
   "metadata": {
    "tags": [
     "large-template"
    ]
   },
   "source": [
    "### Install Cluster-wide Dependencies (Large-scale Template)\n",
    "\n",
    "When running in a distributed Ray Cluster, all nodes need to have access to the installed packages.\n",
    "For this, we'll use `pip install --user` to install the necessary requirements.\n",
    "On an [Anyscale Workspace](https://docs.anyscale.com/user-guide/develop-and-debug/workspaces),\n",
    "this will install packages to a *shared filesystem* that will be available to all nodes in the cluster.\n",
    "\n",
    "```\n",
    "pip install --user -r requirements.txt --upgrade\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c4c3067",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Did you follow the steps above to install dependencies?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 10\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstatsforecast\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StatsForecast\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstatsforecast\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoARIMA, AutoETS\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'statsforecast'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstatsforecast\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoARIMA, AutoETS\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m---> 13\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     14\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDid you follow the steps above to install dependencies?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     15\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Did you follow the steps above to install dependencies?"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pyarrow import parquet as pq\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.air import session\n",
    "\n",
    "try:\n",
    "    from statsforecast import StatsForecast\n",
    "    from statsforecast.models import AutoARIMA, AutoETS\n",
    "except ImportError as e:\n",
    "    raise RuntimeError(\n",
    "        \"Did you follow the steps above to install dependencies?\"\n",
    "    ) from e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167e493d",
   "metadata": {},
   "source": [
    "> ✂️ Replace this value to change the number of data partitions you will use. This will be total the number of Tune trials you will run!\n",
    ">\n",
    "> Note that this template fits two models per data partition and reports the best performing one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bfc8d9",
   "metadata": {
    "tags": [
     "small-template"
    ]
   },
   "outputs": [],
   "source": [
    "NUM_DATA_PARTITIONS: int = 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b1b0a3-09d9-4be5-8682-85dbe1f57347",
   "metadata": {
    "tags": [
     "large-template"
    ]
   },
   "outputs": [],
   "source": [
    "NUM_DATA_PARTITIONS: int = 1000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf104b9",
   "metadata": {},
   "source": [
    "```{tip}\n",
    "If you're running the small-scale version of the template, try setting\n",
    "the number of trials to the recommended number of trials for the large-scale version.\n",
    "It'll be much slower, but you'll see the dramatic speedup once distributing the load\n",
    "to a multi-node Ray cluster in the large-scale version!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607aeb1c",
   "metadata": {},
   "source": [
    "> ✂️ Replace the following with your own data-loading and evaluation helper functions. (Or, just delete these!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91aafaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_m5_partition(unique_id: str) -> pd.DataFrame:\n",
    "    df = (\n",
    "        pq.read_table(\n",
    "            \"s3://anonymous@m5-benchmarks/data/train/target.parquet\",\n",
    "            columns=[\"item_id\", \"timestamp\", \"demand\"],\n",
    "            filters=[(\"item_id\", \"=\", unique_id)],\n",
    "        )\n",
    "        .to_pandas()\n",
    "        .rename(columns={\"item_id\": \"unique_id\", \"timestamp\": \"ds\", \"demand\": \"y\"})\n",
    "    )\n",
    "    df[\"unique_id\"] = df[\"unique_id\"].astype(str)\n",
    "    df[\"ds\"] = pd.to_datetime(df[\"ds\"])\n",
    "    return df.dropna()\n",
    "\n",
    "\n",
    "def evaluate_cross_validation(df, metric):\n",
    "    models = df.drop(columns=[\"ds\", \"cutoff\", \"y\"]).columns.tolist()\n",
    "    evals = []\n",
    "    for model in models:\n",
    "        eval_ = (\n",
    "            df.groupby([\"unique_id\", \"cutoff\"])\n",
    "            .apply(lambda x: metric(x[\"y\"].values, x[model].values))\n",
    "            .to_frame()\n",
    "        )\n",
    "        eval_.columns = [model]\n",
    "        evals.append(eval_)\n",
    "    evals = pd.concat(evals, axis=1)\n",
    "    evals = evals.groupby([\"unique_id\"]).mean(numeric_only=True)\n",
    "    evals[\"best_model\"] = evals.idxmin(axis=1)\n",
    "    return evals\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf544b9",
   "metadata": {},
   "source": [
    "> ✂️ Replace this with your own training logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3884c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_classes = [AutoARIMA, AutoETS]\n",
    "n_windows = 1\n",
    "\n",
    "\n",
    "def train_fn(config: dict):\n",
    "    data_partition_id = config[\"data_partition_id\"]\n",
    "    train_df = get_m5_partition(data_partition_id)\n",
    "\n",
    "    models = [model_cls() for model_cls in model_classes]\n",
    "    forecast_horizon = 4\n",
    "\n",
    "    sf = StatsForecast(\n",
    "        df=train_df,\n",
    "        models=models,\n",
    "        freq=\"D\",\n",
    "        n_jobs=n_windows * len(models),\n",
    "    )\n",
    "    cv_df = sf.cross_validation(\n",
    "        h=forecast_horizon,\n",
    "        step_size=forecast_horizon,\n",
    "        n_windows=n_windows,\n",
    "    )\n",
    "\n",
    "    eval_df = evaluate_cross_validation(df=cv_df, metric=mean_squared_error)\n",
    "    best_model = eval_df[\"best_model\"][data_partition_id]\n",
    "    forecast_mse = eval_df[best_model][data_partition_id]\n",
    "\n",
    "    # Report the best-performing model and its corresponding eval metric.\n",
    "    session.report({\"forecast_mse\": forecast_mse, \"best_model\": best_model})\n",
    "\n",
    "\n",
    "trainable = train_fn\n",
    "trainable = tune.with_resources(\n",
    "    trainable, resources={\"CPU\": len(model_classes) * n_windows}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69204f6",
   "metadata": {},
   "source": [
    "```{note}\n",
    "`tune.with_resources` is used at the end to specify the number of resources to assign *each trial*.\n",
    "Feel free to change this to the resources required by your application! You can also comment out the `tune.with_resources` block to assign `1 CPU` (the default) to each trial.\n",
    "\n",
    "Note that this is purely for Tune to know how many trials to schedule concurrently -- setting the number of CPUs does not actually enforce any kind of resource isolation!\n",
    "```\n",
    "\n",
    "See [Ray Tune's guide on assigning resources](https://docs.ray.io/en/latest/tune/tutorials/tune-resources.html) for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b5d1ab",
   "metadata": {},
   "source": [
    "> ✂️ Replace this with your desired hyperparameter search space!\n",
    ">\n",
    "> For example, this template searches over the data partition ID to train a model on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5e534f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_partitions = list(pd.read_csv(\"item_ids.csv\")[\"item_id\"])\n",
    "if NUM_DATA_PARTITIONS > len(data_partitions):\n",
    "    print(f\"There are only {len(data_partitions)} partitions!\")\n",
    "\n",
    "param_space = {\n",
    "    \"data_partition_id\": tune.grid_search(data_partitions[:NUM_DATA_PARTITIONS]),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b6de04",
   "metadata": {},
   "source": [
    "Run many model training using Ray Tune!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0ea2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = tune.Tuner(trainable, param_space=param_space)\n",
    "result_grid = tuner.fit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e0340b",
   "metadata": {},
   "source": [
    "> ✂️ Replace the metric and mode below with the metric you reported in your training function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e542f956",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_result = result_grid[0]\n",
    "sample_result.metrics\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "265d195fda5292fe8f69c6e37c435a5634a1ed3b6799724e66a975f68fa21517"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
