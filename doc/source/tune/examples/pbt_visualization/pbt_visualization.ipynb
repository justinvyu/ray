{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2438a1d7-6564-4a4d-bb8a-5ae7f3eba552",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Visualizing Population Based Training (PBT) Hyperparameter Optimization\n",
    "\n",
    "This tutorial will go through a simple example that will help you develop a better understanding of what PBT is doing in the backend when using it to tune your algorithms.\n",
    "\n",
    "Assumptions: The reader has a basic understanding of the PBT algorithm and wants to understand and verify the underlying algorithm behavior. [This guide]() gives the necessary background.\n",
    "\n",
    "We will learn how to: **set up checkpointing and loading for PBT** with the functional trainable interface, **configure PBT parameters**, and finally **visualize PBT algorithm behavior.**\n",
    "\n",
    "## Example Code\n",
    "\n",
    "You can find the code we used for this tutorial [here on Github]().\n",
    "\n",
    "## Set up Toy Example\n",
    "\n",
    "The toy example optimization problem we will use comes from the [PBT paper](https://arxiv.org/pdf/1711.09846.pdf) (see Figure 2 for more details). The goal is to find parameters that maximize an (unknown) quadratic function, while only having access to an estimator that depends on the hyperparameters. A practical example of this could be wanting to maximize the (unknown) generalization capabilities of a model across all possible inputs, with only access to the empirical loss of your model, which depends on hyperparameters in order to optimize.\n",
    "\n",
    "\n",
    "\n",
    "We'll start with some imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b2e7ba-532b-431e-aa81-1467cb2b4e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U \"ray[tune]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90471b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.air import session\n",
    "from ray.air.checkpoint import Checkpoint\n",
    "from ray.tune.schedulers import PopulationBasedTraining\n",
    "from ray.tune.tune_config import TuneConfig\n",
    "from ray.air.config import FailureConfig, RunConfig, CheckpointConfig\n",
    "from ray.tune.tuner import Tuner\n",
    "\n",
    "from pbt_visualization_utils import get_init_theta, plot_parameter_history, plot_Q_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a223d6a2-a7d5-40a1-8e12-2a5a1a0a0070",
   "metadata": {},
   "source": [
    "Concretely, we will use the definitions (with very minor modifications) provided in the [paper](https://arxiv.org/pdf/1711.09846.pdf) for the function we are trying to optimize, and the estimator we are given.\n",
    "\n",
    "Here is a list of the concepts we will use for the example, and what they might be analagous to in practice:\n",
    "\n",
    "| Concept within this example | Description | Practical analogy |\n",
    "|---------|-------------|-------------------|\n",
    "|`theta = [theta0, theta1]`|The model parameters that we will update in our training loop.|Neural network parameters|\n",
    "|`h = [h0, h1]`|The hyperparameters that PBT will optimize.|Learning rate, batch size, etc.|\n",
    "|`Q(theta)`|The quadratic function we are trying to maximize.|Generalization capability over all inputs|\n",
    "|`Qhat(theta \\| h)`|The estimator we are given as our training objective, depends (`\\|`) on `h`.|Empirical loss/reward|\n",
    "\n",
    "Below are the implementations in code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75e75db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q(theta):\n",
    "    return 1.2 - (3/4 * theta[0] ** 2 + theta[1] ** 2)\n",
    "\n",
    "def Qhat(params):\n",
    "    theta, h = params[\"theta\"], params[\"h\"]\n",
    "    return 1.2 - (h[0] * np.square(theta[0]) + h[1] * np.square(theta[1]))\n",
    "\n",
    "def grad_Qhat(params):\n",
    "    theta, h = params[\"theta\"], params[\"h\"]\n",
    "    theta_grad = -2 * h * theta\n",
    "    theta_grad[0] *= 3/4\n",
    "    h_grad = -np.square(theta)\n",
    "    h_grad[0] *= 3/4\n",
    "    return {\"theta\": theta_grad, \"h\": h_grad}\n",
    "\n",
    "theta_0 = get_init_theta()\n",
    "print(\"Initial parameter values: theta = \", theta_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee21632-9be6-4f80-ac80-c71696cb0f4f",
   "metadata": {},
   "source": [
    "## Defining the Function Trainable\n",
    "\n",
    "We will define the training loop, which will:\n",
    "1. Load the hyperparameter configuration\n",
    "2. Initialize the model, resuming from a checkpoint if applicable (this is important for PBT, since the scheduler will pause and resume trials frequently).\n",
    "3. Run the training loop and checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1a9fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_func(config):\n",
    "    # Load the hyperparam config passed in by the Tuner\n",
    "    h0 = config.get(\"h0\")\n",
    "    h1 = config.get(\"h1\")\n",
    "    h = np.array([h0, h1]).astype(float)\n",
    "    \n",
    "    lr = config.get(\"lr\")\n",
    "    train_step = 1\n",
    "    checkpoint_interval = config.get(\"checkpoint_interval\", 1)\n",
    "    logging_interval = config.get(\"logging_interval\", 10)\n",
    "    \n",
    "    # Initialize the model parameters\n",
    "    theta = get_init_theta()\n",
    "    \n",
    "    # Load a checkpoint if it exists\n",
    "    # This checkpoint could be a trial's own checkpoint to resume,\n",
    "    # or another trial's checkpoint placed by PBT that we will exploit\n",
    "    if session.get_checkpoint():\n",
    "        checkpoint_dict = session.get_checkpoint().to_dict()\n",
    "        # Load in model (theta)\n",
    "        theta = checkpoint_dict[\"theta\"]\n",
    "        train_step = checkpoint_dict[\"train_step\"] + 1\n",
    "    \n",
    "    # Main training loop (trial stopping handled by Tune scheduler)\n",
    "    while True:\n",
    "        # Perform gradient ascent steps\n",
    "        param_grads = grad_Qhat({\"theta\": theta, \"h\": h})\n",
    "        theta_grad = np.asarray(param_grads[\"theta\"])\n",
    "        theta = theta + lr * theta_grad\n",
    "        \n",
    "        # Checkpoint every `checkpoint_interval` steps\n",
    "        checkpoint = None\n",
    "        should_checkpoint = train_step % checkpoint_interval == 0\n",
    "        if should_checkpoint:\n",
    "            checkpoint = Checkpoint.from_dict({\n",
    "                \"h\": h,\n",
    "                \"train_step\": train_step,\n",
    "                \"theta\": theta,\n",
    "            })\n",
    "\n",
    "        # Define which custom metrics we want in our trial result\n",
    "        result = {\n",
    "            \"Q\": Q(theta),\n",
    "            \"theta0\": theta[0], \"theta1\": theta[1],\n",
    "            \"h0\": h0, \"h1\": h1,\n",
    "            \"train_step\": train_step,\n",
    "        }\n",
    "        \n",
    "        # Report metric for this training iteration, and include the\n",
    "        # trial checkpoint that contains the current parameters if we\n",
    "        # saved it this train step\n",
    "        session.report(result, checkpoint=checkpoint)\n",
    "        \n",
    "        train_step += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa002e2-1d68-404c-84bd-99b8d8119dac",
   "metadata": {},
   "source": [
    "## Configure PBT and Tuner\n",
    "\n",
    "We start by initializing ray (shutting it down if a session existed previously)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68445a3-958f-49a0-a9f9-03121c3c731c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ray.is_initialized():\n",
    "    ray.shutdown()\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155ec478-4f5d-4614-90a5-1197897cbbcf",
   "metadata": {},
   "source": [
    "### Create the PBT scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c517e599-b60a-41cf-b5f4-81a930178b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "perturbation_interval = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7d83d6-ecaf-4975-8b56-6c9cd5443d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "pbt_scheduler = PopulationBasedTraining(\n",
    "    time_attr=\"training_iteration\",\n",
    "    perturbation_interval=perturbation_interval,\n",
    "    quantile_fraction=0.5,\n",
    "    resample_probability=0.5,\n",
    "    hyperparam_mutations={\n",
    "        \"lr\": tune.qloguniform(1e-4, 1e-1, 5e-5),\n",
    "        \"h0\": tune.uniform(0., 1.),\n",
    "        \"h1\": tune.uniform(0., 1.),\n",
    "    },\n",
    "    synch=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8143bd8d-b929-4e27-b965-cf852ba3b3d7",
   "metadata": {},
   "source": [
    "- `time_attr=\"training_iteration\"` in combination with `perturbation_interval=4` will decide every 4 training steps whether a trial should continue or exploit a different trial.\n",
    "- `hyperparam_mutations` specifies `h0` and `h1` to be mutated by PBT and defines the resample distribution for each hyperparameter.\n",
    "- `synch=True` means PBT will run synchronously, which introduces extra overhead, but produces more understandable visualizations for the purposes of this tutorial. Feel free to try changing it to `synch=False` on this notebook to see the asynch behavior!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efe72d7-873d-44c5-9e74-1cd2f41a5c22",
   "metadata": {},
   "source": [
    "### Create the Tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fa9c92-6ccc-4e8c-91ef-04b95af87a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = Tuner(\n",
    "    train_func,\n",
    "    param_space={\n",
    "        # \"lr\": 0.05,\n",
    "        \"lr\": tune.qloguniform(1e-3, 1e-1, 5e-4),\n",
    "        \"h0\": tune.grid_search([0.0, 1.0]),\n",
    "        \"h1\": tune.sample_from(lambda spec: 1. - spec.config[\"h0\"]),\n",
    "        \"num_training_iterations\": 100,\n",
    "        \"checkpoint_interval\": 4,\n",
    "    },\n",
    "    tune_config=TuneConfig(\n",
    "        num_samples=1,\n",
    "        metric=\"Q\", mode=\"max\",\n",
    "        # Set the PBT scheduler in this config\n",
    "        scheduler=pbt_scheduler,\n",
    "    ),\n",
    "    run_config=RunConfig(\n",
    "        stop={\"training_iteration\": 100},\n",
    "        failure_config=FailureConfig(max_failures=3),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7407fba-eb82-4cd6-a9cd-bb2adef451df",
   "metadata": {},
   "source": [
    "```{note}\n",
    "We recommend matching `checkpoint_interval` with `perturbation_interval` from the PBT config.\n",
    "This ensures that the PBT algorithm actually exploits the trials in the most recent iteration.\n",
    "\n",
    "If your `perturbation_interval` is large and want to checkpoint more frequently, set `perturbation_interval` to be a multiple of `checkpoint_interval`.\n",
    "```\n",
    "\n",
    "- `param_space` specifies the *initial* `config` input to our training function. A `grid_search` over two values will launch two trials with a certain set of hyperparameters, and PBT will continue modifying them as training progresses.\n",
    "- The initial hyperparam settings for `h0` are configured so that two trials will spawn, one with `h = [1, 0]` and the other with `h = [0, 1]`. This matches the paper experiment and will be used to compare against a `grid_search` baseline that removes the PBT scheduler."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9221f992-48dc-4cf8-ba9e-3f080a741ee3",
   "metadata": {},
   "source": [
    "## Run the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1559270f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = tuner.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0249f7-9e3e-4f2a-9cf7-b0d6c128fc46",
   "metadata": {},
   "source": [
    "## Visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2af6574",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, axs = plt.subplots(1, 2, figsize=(13, 6))\n",
    "\n",
    "colors = [\"black\", \"red\"]\n",
    "labels = [\"h = [1, 0]\", \"h = [0, 1]\"]\n",
    "\n",
    "plot_parameter_history(\n",
    "    results,\n",
    "    colors,\n",
    "    labels,\n",
    "    perturbation_interval=perturbation_interval,\n",
    "    ax=axs[0]\n",
    ")\n",
    "plot_Q_history(\n",
    "    results,\n",
    "    colors,\n",
    "    labels,\n",
    "    ax=axs[1]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac66d5f0",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1765efa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ray.is_initialized():\n",
    "    ray.shutdown()\n",
    "ray.init()\n",
    "\n",
    "tuner = Tuner(\n",
    "    train_func,\n",
    "    param_space={\n",
    "        \"lr\": 0.05,\n",
    "        \"h0\": tune.grid_search([0.0, 1.0]),\n",
    "        \"h1\": tune.sample_from(lambda spec: 1. - spec.config[\"h0\"]),\n",
    "    },\n",
    "    tune_config=TuneConfig(\n",
    "        num_samples=1, metric=\"Q\", mode=\"max\",\n",
    "    ),\n",
    "    run_config=RunConfig(\n",
    "        stop={\"training_iteration\": 100},\n",
    "        failure_config=FailureConfig(max_failures=3),\n",
    "    ),\n",
    ")\n",
    "\n",
    "results = tuner.fit()\n",
    "if results.errors:\n",
    "    raise RuntimeError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bff9d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, axs = plt.subplots(1, 2, figsize=(13, 6))\n",
    "\n",
    "colors = [\"black\", \"red\"]\n",
    "labels = [\"h = [1, 0]\", \"h = [0, 1]\"]\n",
    "\n",
    "plot_parameter_history(\n",
    "    results,\n",
    "    colors,\n",
    "    labels,\n",
    "    perturbation_interval=perturbation_interval,\n",
    "    ax=axs[0]\n",
    ")\n",
    "plot_Q_history(\n",
    "    results,\n",
    "    colors,\n",
    "    labels,\n",
    "    ax=axs[1]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2daa57-fe86-4f18-9ebb-808b7b449dad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
