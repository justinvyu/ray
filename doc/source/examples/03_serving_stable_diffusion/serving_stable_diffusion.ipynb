{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "379ab4e7",
   "metadata": {},
   "source": [
    "# Serving a Stable Diffusion Model\n",
    "\n",
    "This guide is a quickstart to use [Ray Serve](todo) for model serving. The provided example loads a pretrained stable diffusion model from HuggingFace and serves it to a local endpoint as a Ray Serve deployment. See `server.py` to see what code can be replaced to serve your own models!\n",
    "\n",
    "> Slot in your code below wherever you see the ✂️ icon to build a model serving Ray application off of this template!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e8b48f",
   "metadata": {},
   "source": [
    "## Installing Dependencies\n",
    "\n",
    "First, we'll need to install necessary dependencies in the Anyscale Workspace. To do so, first open up a terminal, and follow one of the following install steps, depending on which size template you picked:\n",
    "\n",
    "### Install Dependencies (Small-scale Template)\n",
    "\n",
    "The small-scale template only runs on a single node (the head node), so we just need to install the requirements *locally*.\n",
    "\n",
    "```\n",
    "pip install -r requirements.txt --upgrade\n",
    "```\n",
    "\n",
    "### Install Cluster-wide Dependencies (Large-scale Template)\n",
    "\n",
    "When running in a distributed Ray Cluster, all nodes need to have access to the installed packages.\n",
    "For this, we'll use `pip install --user` to install the necessary requirements.\n",
    "On an [Anyscale Workspace](https://docs.anyscale.com/user-guide/develop-and-debug/workspaces),\n",
    "this will install packages to a *shared filesystem* that will be available to all nodes in the cluster.\n",
    "\n",
    "```\n",
    "pip install --user -r requirements.txt --upgrade\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13a96fb",
   "metadata": {},
   "source": [
    "## Deploy the Ray Serve application locally\n",
    "\n",
    "The Ray Serve application with the model serving logic can be found in `app.py`, where we define:\n",
    "- The `/imagine` API endpoint that we query to generate the image.\n",
    "- The stable diffusion model loaded inside a Ray Serve Deployment.\n",
    "  We'll specify the *number of model replicas* to keep active in our Ray cluster. These model replicas can process incoming requests concurrently.\n",
    "\n",
    "Let's deploy the Ray Serve application locally (at `http://localhost:8000`)!\n",
    "Open a terminal in your Workspace, and run the following command in your workspace directory (where `server.py` is located):\n",
    "\n",
    "\n",
    "| Template Size | Launch Command |\n",
    "| ------------- | --------------------- |\n",
    "|Small-scale (single-node) | `python server.py --num-replicas=1`  |\n",
    "|Large-scale (multi-node)  | `python server.py --num-replicas=4` |\n",
    "\n",
    "This command will continue running to host your local Ray Serve application.\n",
    "This will be the place to view all the autoscaling logs, as well as any logs emitted by\n",
    "the model inference once requests start coming through."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2724ec",
   "metadata": {},
   "source": [
    "## Make a Request\n",
    "\n",
    "Next, we'll build a simple client to submit prompts as HTTP requests to the local endpoint at `http://localhost:8000/imagine`.\n",
    "The script will "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e48522f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import aiohttp\n",
    "import asyncio\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "import uuid\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4d5d37fc",
   "metadata": {},
   "source": [
    "> ✂️ Replace this value to change the number of images to generate per prompt.\n",
    ">\n",
    "> Each image will be generated starting from a different set of random noise,\n",
    "> so you'll be able to see multiple options per prompt!\n",
    "> With more model replicas, more images can be generated in parallel.\n",
    ">\n",
    "> Try starting with the recommended value in the table below:\n",
    "\n",
    "| Template Size | `NUM_IMAGES_PER_PROMPT` |\n",
    "| ------------- | --------------------- |\n",
    "|Small-scale (single-node) | `1` |\n",
    "|Large-scale (multi-node)  | `4` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdb98fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_IMAGES_PER_PROMPT: int = 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69dfe88",
   "metadata": {},
   "source": [
    "Start the client script in the next few cells, and enter a prompt to generate your first image! For example:\n",
    "\n",
    "```\n",
    "Enter a prompt (or 'q' to quit):   twin peaks sf in basquiat painting style\n",
    "\n",
    "Generating image(s)...\n",
    "(Take a look at the terminal serving the endpoint for more logs!)\n",
    "\n",
    "\n",
    "Generated 1 image(s) in 69.89 seconds to the directory: 58b298d9\n",
    "```\n",
    "\n",
    "![Example output](https://user-images.githubusercontent.com/3887863/221063452-3c5e5f6b-fc8c-410f-ad5c-202441cceb51.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d6c02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = \"http://localhost:8000/imagine\"\n",
    "\n",
    "\n",
    "async def generate_image(session, prompt):\n",
    "    req = {\"prompt\": prompt, \"img_size\": 776}\n",
    "    async with session.get(endpoint, params=req) as resp:\n",
    "        image_data = await resp.read()\n",
    "    return image_data\n",
    "\n",
    "\n",
    "def show_images(filenames):\n",
    "    fig, axs = plt.subplots(1, len(filenames), figsize=(4 * len(filenames), 4))\n",
    "    for i, filename in enumerate(filenames):\n",
    "        ax = axs if len(filenames) == 1 else axs[i]\n",
    "        ax.imshow(plt.imread(filename))\n",
    "        ax.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "async def main():\n",
    "    try:\n",
    "        requests.get(endpoint, timeout=0.1)\n",
    "    except Exception as e:\n",
    "        raise RuntimeWarning(\n",
    "            \"Did you setup the Ray Serve model replicas with \"\n",
    "            \"`python server.py --num-replicas=...` in another terminal yet?\"\n",
    "        ) from e\n",
    "\n",
    "    while True:\n",
    "        prompt = input(f\"\\nEnter a prompt (or 'q' to quit):  \")\n",
    "        if prompt.lower() == \"q\":\n",
    "            break\n",
    "\n",
    "        print(\"\\nGenerating image(s)...\")\n",
    "        print(\"(Take a look at the terminal serving the endpoint for more logs!)\\n\")\n",
    "        start = time.time()\n",
    "\n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            tasks = []\n",
    "            for i in range(NUM_IMAGES_PER_PROMPT):\n",
    "                tasks.append(generate_image(session, prompt))\n",
    "            images = await asyncio.gather(*tasks)\n",
    "\n",
    "        dirname = f\"{uuid.uuid4().hex[:8]}\"\n",
    "        os.makedirs(dirname)\n",
    "        filenames = []\n",
    "        for i, image in enumerate(images):\n",
    "            filename = os.path.join(dirname, f\"{i}.png\")\n",
    "            with open(filename, \"wb\") as f:\n",
    "                f.write(image)\n",
    "            filenames.append(filename)\n",
    "\n",
    "        elapsed = time.time() - start\n",
    "\n",
    "        print(\n",
    "            f\"\\nGenerated {len(images)} image(s) in {elapsed:.2f} seconds to \"\n",
    "            f\"the directory: {dirname}\\n\"\n",
    "        )\n",
    "        show_images(filenames)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588e7cae",
   "metadata": {},
   "source": [
    "Once the stable diffusion model finishes generating your image, it will be included in the HTTP response body.\n",
    "The client writes this to an image in your Workspace directory for you to view. It'll also show up in the notebook cell!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb466c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "await main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290d3299",
   "metadata": {},
   "source": [
    "You've successfully served a stable diffusion model!\n",
    "You can modify this template and quickly iterate your model deployment directly on your cluster within your Anyscale Workspace,\n",
    "testing with the local endpoint."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
