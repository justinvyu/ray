{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25ea6871-261f-4ee7-9263-42c93dcb8857",
   "metadata": {},
   "source": [
    "# Batch Inference with Ray Data\n",
    "\n",
    "This template is a quickstart to using [Ray Data](datasets) for batch inference. Ray Data is one of many libraries under the [Ray AI Runtime](air). See [this blog post](https://www.anyscale.com/blog/model-batch-inference-in-ray-actors-actorpool-and-datasets) for more information on why and how you should perform batch inference with Ray!\n",
    "\n",
    "This template walks through GPU batch prediction on an image dataset using a PyTorch model, but the framework and data format are there just to help you build your own application!\n",
    "\n",
    "At a high level, this template will:\n",
    "1. [Load your dataset using Ray Data.](creating_datasets)\n",
    "2. [Preprocess your dataset before feeding it to your model.](transforming_datasets)\n",
    "3. [Initialize your model and perform inference on a shard of your dataset with a remote actor.](transform_datasets_callable_classes)\n",
    "4. [Save your prediction results.](input-output)\n",
    "\n",
    "> Slot in your code below wherever you see the ✂️ icon to build a many model training Ray application off of this template!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1fb575-9b04-4815-8312-78db50f24552",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from typing import Dict\n",
    "\n",
    "import ray\n",
    "from ray.data.datasource.partitioning import Partitioning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b67e88-6f28-4ccf-952d-4c10736051f2",
   "metadata": {},
   "source": [
    ">✂️ Replace these values depending on the template size you picked!\n",
    ">\n",
    ">For example, for the larger scale template with 4 nodes, each with 1 GPU, you may want to use 4 workers, each using 1 GPU."
   ]
  },
  {
   "cell_type": "raw",
   "id": "71de07cc-6533-4594-b822-63d2afb4d196",
   "metadata": {},
   "source": [
    "NUM_WORKERS: int = 1\n",
    "\n",
    "USE_GPU: bool = True\n",
    "NUM_GPUS_PER_WORKER: float = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c838f4-00a6-4e20-92eb-ee883173fbd0",
   "metadata": {},
   "source": [
    "```{tip}\n",
    "See the resources available in your Ray cluster with the Ray State command line interface!\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5592d447-5d85-4f7d-bb49-43feed952bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ray status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c46c377-b9a5-4992-b0f0-908380716d90",
   "metadata": {},
   "source": [
    "```{tip}\n",
    "Try setting `NUM_GPUS_PER_WORKER` to a fractional amount! This will leverage Ray's fractional resource allocation, which means you can schedule multiple batch inference workers to happen on the same GPU.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed3c564-5fd8-439e-864c-943c3cb66030",
   "metadata": {},
   "source": [
    "> ✂️ Replace this function with logic to load your own data with Ray Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8a5e3e-484e-43fb-b271-418b9c22178f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ray_dataset() -> ray.data.Dataset:\n",
    "    s3_uri = \"s3://anonymous@air-example-data-2/imagenette2/val/\"\n",
    "    partitioning = Partitioning(\"dir\", field_names=[\"class\"], base_dir=s3_uri)\n",
    "    ds = ray.data.read_images(\n",
    "        s3_uri, size=(256, 256), partitioning=partitioning, mode=\"RGB\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eba3f32-8147-4eaf-ae63-445afa95a0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_ray_dataset()\n",
    "ds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0200e3-6299-4f24-8c80-bac574411ef8",
   "metadata": {},
   "source": [
    "> ✂️ Replace this function with your own data preprocessing logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2475ab18-320e-42e5-9775-1887b21a6ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(batch: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n",
    "    def to_tensor(batch: np.ndarray) -> torch.Tensor:\n",
    "        tensor = torch.as_tensor(batch, dtype=torch.float)\n",
    "        # (B, H, W, C) -> (B, C, H, W)\n",
    "        tensor = tensor.permute(0, 3, 1, 2).contiguous()\n",
    "        # [0., 255.] -> [0., 1.]\n",
    "        tensor = tensor.div(255)\n",
    "        return tensor\n",
    "\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Lambda(to_tensor),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "    )\n",
    "    return {\"image\": transform(batch[\"image\"]).numpy()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ccc393-fea3-4a86-8b5e-496322a54367",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.map_batches(fn=preprocess, batch_format=\"numpy\")\n",
    "ds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f537c125-38f4-4a2f-a560-697b71099d9e",
   "metadata": {},
   "source": [
    "> ✂️ Replace parts of this Callable class with your own model initialization and inference logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d3959d-511e-4a17-b810-fbe8c6bdd061",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictCallable:\n",
    "    def __init__(self):\n",
    "        # <Replace this with your own model initialization>\n",
    "        from torchvision import models\n",
    "\n",
    "        self.model = models.resnet152(pretrained=True)\n",
    "        self.model.eval()\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def __call__(self, batch: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n",
    "        # <Replace this with your own model inference logic>\n",
    "        input_data = torch.as_tensor(batch[\"image\"], device=self.device)\n",
    "        with torch.no_grad():\n",
    "            result = self.model(input_data)\n",
    "        return {\"predictions\": result.cpu().numpy()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2c5792-a0cc-4da9-b868-aecaf8ccb69e",
   "metadata": {},
   "source": [
    "Now, perform batch prediction using Ray Data! Ray Data will perform model inference using `NUM_WORKERS` copies of the `PredictCallable` class you defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51438c89-6307-4cf6-bc73-b3ab4ca80246",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = ds.map_batches(\n",
    "    PredictCallable,\n",
    "    batch_size=128,\n",
    "    compute=ray.data.ActorPoolStrategy(\n",
    "        # Fix the number of batch inference workers to a specified value.\n",
    "        min_size=args.num_workers,\n",
    "        max_size=args.num_workers,\n",
    "    ),\n",
    "    num_gpus=NUM_GPUS_PER_WORKER if USE_GPU else 0,\n",
    "    batch_format=\"numpy\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7988cd-8f1a-4f43-80b9-3af080ef6025",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.take(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05d6127-3eae-43d2-aa25-f6968d6d6054",
   "metadata": {},
   "source": [
    "```{tip}\n",
    "Play around with the `min_size` and `max_size` parameters below to enable autoscaling!\n",
    "For example, try commenting out `max_size`: this will autoscale up to an infinite number of workers, given the number of resources in the cluster.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d46dc1-75fe-4ce3-a02b-2f9915b4a1db",
   "metadata": {},
   "source": [
    "Shard the predictions into a few partitions, and save each partition to a file!\n",
    "\n",
    "```{note}\n",
    "This currently saves to the local filesystem under `/tmp/predictions`, but you could also save to a cloud bucket (e.g., `s3://predictions-bucket`).\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea21cb3c-fa0f-4da6-9edd-bf23dc0a1f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_shards = 3\n",
    "predictions.repartition(num_shards).write_parquet(\"local:///tmp/predictions\")\n",
    "print(\"Predictions saved to `/tmp/predictions`!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "265d195fda5292fe8f69c6e37c435a5634a1ed3b6799724e66a975f68fa21517"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
