{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling Batch Inference with Ray Data\n",
    "\n",
    "| Template Specification | Description |\n",
    "| ---------------------- | ----------- |\n",
    "| Summary | This template walks through GPU batch inference on an image dataset. |\n",
    "| Time to Run | Less than 2 minutes to compute predictions on the dataset. |\n",
    "| Minimum Compute Requirements | No hard requirements. The default is 4 nodes, each with 1 NVIDIA T4 GPU. |\n",
    "| Cluster Environment | This template uses the latest Anyscale-provided Ray ML image using Python 3.9: [`anyscale/ray-ml:latest-py39-gpu`](https://docs.anyscale.com/reference/base-images/overview). If you want to change to a different cluster environment, make sure that it is based off of this image! |\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "MNwt9bSG0hin"
   },
   "source": [
    "In this example, we will introduce how to use the [Ray Data](https://docs.ray.io/en/latest/data/data.html) for **large-scale image classification batch inference with multiple GPU workers.**\n",
    "\n",
    "In particular, we will:\n",
    "- Load Imagenette dataset from S3 bucket and create a [Ray `Dataset`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.html).\n",
    "- Load a pretrained Vision Transformer from Huggingface that's been trained on ImageNet.\n",
    "- Use [Ray Data](https://docs.ray.io/en/latest/data/data.html) to preprocess the dataset and do model inference parallelizing across multiple GPUs\n",
    "- Evaluate the predictions and save results to S3/local disk.\n",
    "\n",
    "This example will still work even if you do not have GPUs available, but overall performance will be slower."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default cluster environment used by this template already has all the dependencies\n",
    "needed to run. If you're using a custom cluster environment, you'll need to install\n",
    "`transformers` and restart the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Reading the Dataset from S3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Imagenette](https://github.com/fastai/imagenette) is a subset of Imagenet with 10 classes. We have this dataset hosted publicly in an S3 bucket. Since we are only doing inference here, we load in just the validation split.\n",
    "\n",
    "Here, we use {meth}`ray.data.read_images <ray.data.read_images>` to load the validation set from S3. Ray Data also supports reading from a variety of other [datasources and formats](https://docs.ray.io/en/latest/data/loading-data.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "217255c5a2ba4ec5890f6f3667f5b429"
     ]
    },
    "id": "6i15qjnH0hin",
    "outputId": "c22aaba0-b33a-40f5-cf89-a70847098af2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import ray\n",
    "\n",
    "s3_uri = \"s3://anonymous@air-example-data-2/imagenette2/val/\"\n",
    "\n",
    "ds = ray.data.read_images(s3_uri, mode=\"RGB\")\n",
    "ds\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspecting the schema, we can see that there is 1 column in the dataset containing the images stored as Numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds.schema()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Inference on a single batch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can do inference on a single batch of data, using a pre-trained Vision Transformer from Huggingface following [this Huggingface example](https://huggingface.co/docs/transformers/tasks/image_classification#inference). \n",
    "\n",
    "Let’s get a batch of 10 from our dataset. Each image in the batch is represented as a Numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "single_batch = ds.take_batch(10)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize 1 image from this batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "img = Image.fromarray(single_batch[\"image\"][0])\n",
    "img\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let’s create a Huggingface Image Classification pipeline from a pre-trained Vision Transformer model.\n",
    "\n",
    "We specify the following configurations:\n",
    "1. Set the device to \"cuda:0\" to use GPU for inference\n",
    "2. We set the batch size to 10 so that we can maximize GPU utilization and do inference on the entire batch at once. \n",
    "\n",
    "We also convert the image Numpy arrays into PIL Images since that's what Huggingface expects.\n",
    "\n",
    "From the results, we see that all of the images in the batch are correctly being classified as \"tench\" which is a type of fish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from PIL import Image\n",
    "\n",
    "# If doing CPU inference, set device=\"cpu\" instead.\n",
    "classifier = pipeline(\n",
    "    \"image-classification\", model=\"google/vit-base-patch16-224\", device=\"cuda:0\"\n",
    ")\n",
    "outputs = classifier(\n",
    "    [Image.fromarray(image_array) for image_array in single_batch[\"image\"]],\n",
    "    top_k=1,\n",
    "    batch_size=10,\n",
    ")\n",
    "del classifier  # Delete the classifier to free up GPU memory.\n",
    "outputs\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Scaling up to the full Dataset with Ray Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using Ray Data, we can apply the same logic in the previous section to scale up to the entire dataset, leveraging all the GPUs in our cluster."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a couple unique properties about the inference step:\n",
    "1. Model initialization is usually pretty expensive\n",
    "2. We want to do inference in batches to maximize GPU utilization.\n",
    "\n",
    "\n",
    "To address 1, we package the inference code in a `ImageClassifier` class. Using a class allows us to put the expensive pipeline loading and initialization code in the `__init__` constructor, which will run only once. \n",
    "The actual model inference logic is in the `__call__` method, which will be called for each batch.\n",
    "\n",
    "To address 2, we do our inference in batches, specifying a `batch_size` to the Huggingface Pipeline.\n",
    "The `__call__` method takes a batch of data items, instead of a single one. \n",
    "In this case, the batch is a dict that has one key named \"image\", and the value is a Numpy array of images represented in `np.ndarray` format. This is the same format in step 2, and we can reuse the same inferencing logic from step 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "import numpy as np\n",
    "\n",
    "from transformers import pipeline\n",
    "from PIL import Image\n",
    "\n",
    "# Pick the largest batch size that can fit on our GPUs\n",
    "BATCH_SIZE = 1024\n",
    "\n",
    "\n",
    "class ImageClassifier:\n",
    "    def __init__(self):\n",
    "        # If doing CPU inference, set `device=\"cpu\"` instead.\n",
    "        self.classifier = pipeline(\n",
    "            \"image-classification\", model=\"google/vit-base-patch16-224\", device=\"cuda:0\"\n",
    "        )\n",
    "\n",
    "    def __call__(self, batch: Dict[str, np.ndarray]):\n",
    "        # Convert the numpy array of images into a list of PIL images which is the format the HF pipeline expects.\n",
    "        outputs = self.classifier(\n",
    "            [Image.fromarray(image_array) for image_array in batch[\"image\"]],\n",
    "            top_k=1,\n",
    "            batch_size=BATCH_SIZE,\n",
    "        )\n",
    "\n",
    "        # `outputs` is a list of length-one lists. For example:\n",
    "        # [[{'score': '...', 'label': '...'}], ..., [{'score': '...', 'label': '...'}]]\n",
    "        batch[\"score\"] = [output[0][\"score\"] for output in outputs]\n",
    "        batch[\"label\"] = [output[0][\"label\"] for output in outputs]\n",
    "        return batch\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we use the [`map_batches`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.map_batches.html) API to apply the model to the whole dataset.\n",
    "\n",
    "The first parameter of `map_batches` is the user-defined function (UDF), which can either be a function or a class. Since we are using a class in this case, the UDF will run as long-running [Ray actors](https://docs.ray.io/en/latest/ray-core/key-concepts.html#actors). For class-based UDFs, we use the `compute` argument to specify [`ActorPoolStrategy`](https://docs.ray.io/en/latest/data/api/doc/ray.data.ActorPoolStrategy.html) with the number of parallel actors. And the `batch_size` argument indicates the number of images in each batch.\n",
    "\n",
    "The `num_gpus` argument specifies the number of GPUs needed for each `ImageClassifier` instance. In this case, we want 1 GPU for each model replica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions = ds.map_batches(\n",
    "    ImageClassifier,\n",
    "    compute=ray.data.ActorPoolStrategy(\n",
    "        size=4\n",
    "    ),  # Use 4 GPUs. Change this number based on the number of GPUs in your cluster.\n",
    "    num_gpus=1,  # Specify 1 GPU per model replica.\n",
    "    batch_size=BATCH_SIZE,  # Use the largest batch size that can fit on our GPUs\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify and Save Results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a small batch and verify the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prediction_batch = predictions.take_batch(5)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that all the images are correctly classified as \"tench\", which is a type of fish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "for image, prediction in zip(prediction_batch[\"image\"], prediction_batch[\"label\"]):\n",
    "    img = Image.fromarray(image)\n",
    "    display(img)\n",
    "    print(\"Label: \", prediction)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the samples look good, we can proceed with saving the results to an external storage, e.g., S3 or local disks. See [Ray Data Input/Output](https://docs.ray.io/en/latest/data/api/input_output.html) for all supported storages and file formats.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "\n",
    "temp_dir = tempfile.mkdtemp()\n",
    "\n",
    "# The `local://` prefix is need to make sure all results get written on the head node.\n",
    "predictions.write_parquet(f\"local://{temp_dir}\")\n",
    "print(f\"Predictions saved to `{temp_dir}`!\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "a8c1140d108077f4faeb76b2438f85e4ed675f93d004359552883616a1acd54c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
