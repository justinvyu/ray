{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dde8e978",
   "metadata": {},
   "source": [
    "# Serving a Stable Diffusion Model\n",
    "\n",
    "This guide is a quickstart to use [Ray Serve](https://docs.ray.io/en/latest/serve/index.html) for model serving. Ray Serve is one of many libraries under the [Ray AI Runtime](https://docs.ray.io/en/latest/ray-air/getting-started.html).\n",
    "\n",
    "This template loads a pretrained stable diffusion model from HuggingFace and serves it to a local endpoint as a Ray Serve deployment. \n",
    "\n",
    "> Slot in your code below wherever you see the ✂️ icon to build a model serving Ray application off of this template!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06dadcdb",
   "metadata": {},
   "source": [
    "## Installing Dependencies\n",
    "\n",
    "First, we'll need to install necessary dependencies in the Anyscale Workspace. To do so, first open up a terminal, and follow one of the following install steps, depending on which size template you picked:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd4dc50-2d3a-485b-ae8e-01fc7274d18d",
   "metadata": {
    "tags": [
     "small"
    ]
   },
   "source": [
    "### Install Dependencies (Small-scale Template)\n",
    "\n",
    "The small-scale template only runs on a single node (the head node), so we just need to install the requirements *locally*.\n",
    "\n",
    "```\n",
    "pip install -r requirements.txt --upgrade\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c517c56-e2bb-41e0-990f-1db272a955e9",
   "metadata": {
    "tags": [
     "large"
    ]
   },
   "source": [
    "### Install Cluster-wide Dependencies (Large-scale Template)\n",
    "\n",
    "When running in a distributed Ray Cluster, all nodes need to have access to the installed packages.\n",
    "For this, we'll use `pip install --user` to install the necessary requirements.\n",
    "On an [Anyscale Workspace](https://docs.anyscale.com/user-guide/develop-and-debug/workspaces),\n",
    "this will install packages to a *shared filesystem* that will be available to all nodes in the cluster.\n",
    "\n",
    "```\n",
    "pip install --user -r requirements.txt --upgrade\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da58ec6",
   "metadata": {},
   "source": [
    "## Deploy the Ray Serve application locally\n",
    "\n",
    "First, we define the Ray Serve application with the model loading and inference logic. This includes setting up:\n",
    "- The `/imagine` API endpoint that we query to generate the image.\n",
    "- The stable diffusion model loaded inside a Ray Serve Deployment.\n",
    "  We'll specify the *number of model replicas* to keep active in our Ray cluster. These model replicas can process incoming requests concurrently.\n",
    "\n",
    "<!-- Open a terminal in your Workspace, and run the following command in your workspace directory (where `server.py` is located):\n",
    "\n",
    "\n",
    "| Template Size | Launch Command |\n",
    "| ------------- | --------------------- |\n",
    "|Small-scale (single-node) | `python server.py --num-replicas=1`  |\n",
    "|Large-scale (multi-node)  | `python server.py --num-replicas=4` |\n",
    "\n",
    "This command will continue running to host your local Ray Serve application.\n",
    "This will be the place to view all the autoscaling logs, as well as any logs emitted by\n",
    "the model inference once requests start coming through. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b16a769",
   "metadata": {},
   "outputs": [],
   "source": [
    "import aiohttp\n",
    "import asyncio\n",
    "from fastapi import FastAPI\n",
    "from fastapi.responses import Response\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "import uuid\n",
    "\n",
    "from ray import serve\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    from diffusers import EulerDiscreteScheduler, StableDiffusionPipeline\n",
    "except ImportError as e:\n",
    "    raise RuntimeError(\n",
    "        \"Did you follow the steps above to install dependencies?\"\n",
    "    ) from e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c03aef5",
   "metadata": {},
   "source": [
    "> ✂️ Replace these values to change the number of model replicas to serve, as well as the GPU resources required by each replica.\n",
    ">\n",
    "> With more model replicas, more images can be generated in parallel!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea95e656",
   "metadata": {
    "tags": [
     "small"
    ]
   },
   "outputs": [],
   "source": [
    "NUM_REPLICAS: int = 1\n",
    "NUM_GPUS_PER_REPLICA: float = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aeb3933-4ab4-4e1a-9b42-0d0c2d336e31",
   "metadata": {
    "tags": [
     "large"
    ]
   },
   "outputs": [],
   "source": [
    "NUM_REPLICAS: int = 4\n",
    "NUM_GPUS_PER_REPLICA: float = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480570f3",
   "metadata": {},
   "source": [
    "First, we'll define the Ray Serve Deployment, which will load and perform inference with a stable diffusion model.\n",
    "\n",
    "> ✂️ Modify this block to load your own model, and change the `generate` method to perform your own online inference logic!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823a64c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@serve.deployment(\n",
    "    ray_actor_options={\"num_gpus\": NUM_GPUS_PER_REPLICA},\n",
    "    num_replicas=NUM_REPLICAS,\n",
    ")\n",
    "class StableDiffusionV2:\n",
    "    def __init__(self):\n",
    "        # <Replace with your own model loading logic>\n",
    "        model_id = \"stabilityai/stable-diffusion-2\"\n",
    "        scheduler = EulerDiscreteScheduler.from_pretrained(\n",
    "            model_id, subfolder=\"scheduler\"\n",
    "        )\n",
    "        self.pipe = StableDiffusionPipeline.from_pretrained(\n",
    "            model_id, scheduler=scheduler, revision=\"fp16\", torch_dtype=torch.float16\n",
    "        )\n",
    "        self.pipe = self.pipe.to(\"cuda\")\n",
    "\n",
    "    def generate(self, prompt: str, img_size: int = 512):\n",
    "        # <Replace with your own model inference logic>\n",
    "        assert len(prompt), \"prompt parameter cannot be empty\"\n",
    "        image = self.pipe(prompt, height=img_size, width=img_size).images[0]\n",
    "        return image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790b4423",
   "metadata": {},
   "source": [
    "Next, we'll define the actual API endpoint to live at `/imagine`.\n",
    "\n",
    "> ✂️ Modify this block to change the endpoint URL, response schema, and add any post-processing logic needed from your model output!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b4fad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = FastAPI()\n",
    "\n",
    "\n",
    "@serve.deployment(num_replicas=1, route_prefix=\"/\")\n",
    "@serve.ingress(app)\n",
    "class APIIngress:\n",
    "    def __init__(self, diffusion_model_handle) -> None:\n",
    "        self.handle = diffusion_model_handle\n",
    "\n",
    "    @app.get(\n",
    "        \"/imagine\",\n",
    "        responses={200: {\"content\": {\"image/png\": {}}}},\n",
    "        response_class=Response,\n",
    "    )\n",
    "    async def generate(self, prompt: str, img_size: int = 512):\n",
    "        assert len(prompt), \"prompt parameter cannot be empty\"\n",
    "\n",
    "        image = await (await self.handle.generate.remote(prompt, img_size=img_size))\n",
    "\n",
    "        file_stream = BytesIO()\n",
    "        image.save(file_stream, \"PNG\")\n",
    "        return Response(content=file_stream.getvalue(), media_type=\"image/png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7f6afe",
   "metadata": {},
   "source": [
    "Now, we deploy the Ray Serve application locally at `http://localhost:8000`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68861c40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "entrypoint = APIIngress.bind(StableDiffusionV2.bind())\n",
    "port = 8000\n",
    "\n",
    "# Shutdown any existing Serve replicas, if they're still around.\n",
    "serve.shutdown()\n",
    "serve.run(entrypoint, port=port, name=\"serving_stable_diffusion_template\")\n",
    "print(\"Done setting up replicas! Now accepting requests...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18697523",
   "metadata": {},
   "source": [
    "## Make requests to the endpoint\n",
    "\n",
    "Next, we'll build a simple client to submit prompts as HTTP requests to the local endpoint at `http://localhost:8000/imagine`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4d8be0",
   "metadata": {},
   "source": [
    "> ✂️ Replace this value to change the number of images to generate per prompt.\n",
    ">\n",
    "> Each image will be generated starting from a different set of random noise,\n",
    "> so you'll be able to see multiple options per prompt!\n",
    ">\n",
    "> Try starting with `NUM_IMAGES_PER_PROMPT` equal to `NUM_REPLICAS` from earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba43314",
   "metadata": {
    "tags": [
     "small"
    ]
   },
   "outputs": [],
   "source": [
    "NUM_IMAGES_PER_PROMPT: int = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbd9b32-f302-4b5d-86ea-3a8d91b968a4",
   "metadata": {
    "tags": [
     "large"
    ]
   },
   "outputs": [],
   "source": [
    "NUM_IMAGES_PER_PROMPT: int = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0758652",
   "metadata": {},
   "source": [
    "> ✂️ You can choose to run this interactively, or submit a single `PROMPT`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848d4b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "INTERACTIVE: bool = False\n",
    "PROMPT = \"twin peaks sf in basquiat painting style\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3844ce94",
   "metadata": {},
   "source": [
    "Start the client script in the next few cells, and generate your first image! For example:\n",
    "\n",
    "If running interactively, this will look like:\n",
    "\n",
    "```\n",
    "Enter a prompt (or 'q' to quit):   twin peaks sf in basquiat painting style\n",
    "\n",
    "Generating image(s)...\n",
    "(Take a look at the terminal serving the endpoint for more logs!)\n",
    "\n",
    "\n",
    "Generated 1 image(s) in 69.89 seconds to the directory: 58b298d9\n",
    "```\n",
    "\n",
    "![Example output](https://user-images.githubusercontent.com/3887863/221063452-3c5e5f6b-fc8c-410f-ad5c-202441cceb51.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0486295c",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = f\"http://localhost:{port}/imagine\"\n",
    "\n",
    "\n",
    "async def generate_image(session, prompt):\n",
    "    req = {\"prompt\": prompt, \"img_size\": 776}\n",
    "    async with session.get(endpoint, params=req) as resp:\n",
    "        image_data = await resp.read()\n",
    "    return image_data\n",
    "\n",
    "\n",
    "def show_images(filenames):\n",
    "    fig, axs = plt.subplots(1, len(filenames), figsize=(4 * len(filenames), 4))\n",
    "    for i, filename in enumerate(filenames):\n",
    "        ax = axs if len(filenames) == 1 else axs[i]\n",
    "        ax.imshow(plt.imread(filename))\n",
    "        ax.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "async def main():\n",
    "    try:\n",
    "        requests.get(endpoint, timeout=0.1)\n",
    "    except Exception as e:\n",
    "        raise RuntimeWarning(\n",
    "            \"Did you setup the Ray Serve model replicas with \"\n",
    "            \"`python server.py --num-replicas=...` in another terminal yet?\"\n",
    "        ) from e\n",
    "\n",
    "    while True:\n",
    "        prompt = PROMPT if not INTERACTIVE else input(f\"\\nEnter a prompt (or 'q' to quit):  \")\n",
    "        if prompt.lower() == \"q\":\n",
    "            break\n",
    "\n",
    "        print(\"\\nGenerating image(s)...\\n\")\n",
    "        start = time.time()\n",
    "\n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            tasks = []\n",
    "            for i in range(NUM_IMAGES_PER_PROMPT):\n",
    "                tasks.append(generate_image(session, prompt))\n",
    "            images = await asyncio.gather(*tasks)\n",
    "\n",
    "        dirname = f\"{uuid.uuid4().hex[:8]}\"\n",
    "        os.makedirs(dirname)\n",
    "        filenames = []\n",
    "        for i, image in enumerate(images):\n",
    "            filename = os.path.join(dirname, f\"{i}.png\")\n",
    "            with open(filename, \"wb\") as f:\n",
    "                f.write(image)\n",
    "            filenames.append(filename)\n",
    "\n",
    "        elapsed = time.time() - start\n",
    "        print(\n",
    "            f\"\\nGenerated {len(images)} image(s) in {elapsed:.2f} seconds to \"\n",
    "            f\"the directory: {dirname}\\n\"\n",
    "        )\n",
    "        show_images(filenames)\n",
    "        if not INTERACTIVE:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3762d543",
   "metadata": {},
   "source": [
    "Once the stable diffusion model finishes generating your image, it will be included in the HTTP response body.\n",
    "The client writes this to an image in your Workspace directory for you to view. It'll also show up in the notebook cell!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9565f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "await main()\n",
    "serve.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773a42ed",
   "metadata": {},
   "source": [
    "You've successfully served a stable diffusion model!\n",
    "You can modify this template and quickly iterate your model deployment directly on your cluster within your Anyscale Workspace,\n",
    "testing with the local endpoint."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
